# ğŸ§  Auto-ml tool & common cloud storage interface

A modular, production-ready machine learning workflows library for general purpose modeling on tabular dataset.

This repository combines best practices in machine learning engineering â€” data acquisition from kaggle , data pre-processing, model training and evaluation, tracking with MLflow, and rigorous code quality with pre-commit hooks and unit testing.
The final working package can be installed as .whl binary and is shared along with the github repo.

---

## ğŸ“š Table of Contents

- [ğŸ”§ Features](#-features)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸš€ Getting Started](#-getting-started)
- [ğŸ‹ï¸â€â™‚ï¸ Training the Model](#ï¸-training-the-model)
- [ğŸ§ª Running Tests](#-running-tests)
- [ğŸŒ Serving via Web App](#-serving-via-web-app)
- [ğŸ“¦ Docker Deployment](#-docker-deployment)
- [ğŸ“Š MLflow Integration](#-mlflow-integration)
- [ğŸ’¡ API Usage Example](#-api-usage-example)
- [ğŸ§° Development Practices](#-development-practices)
- [ğŸ“ Notebooks & Data](#-notebooks--data)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ“„ License](#-license)

---

## ğŸ”§ Features

- âœ… End-to-end ML prediction (both regression & classification) workflow
- âœ… Clean, modular Python codebase
- âœ… MLflow for experiment tracking
- âœ… Docker support for reproducible environments
- âœ… Pre-commit hooks (Black, Flake8, isort, etc.)
- âœ… Pytest unit testing
- âœ… Logging and configuration management

---

## ğŸ“ Project Structure

```text
src/
â””â”€â”€ mlutils/
    â”œâ”€â”€ cloud
    â”‚   â”œâ”€â”€ azure.py            # Azure
    â”‚   â”œâ”€â”€ aws.py            # Amazon Web Services
    â”‚   â”œâ”€â”€ gcp.py            # Google Cloud Platform
    â”‚
    â”œâ”€â”€ automl/
    â”‚   â”œâ”€â”€ train.py          # Model training logic
    â”‚   â”œâ”€â”€ eval.py           # Evaluation metrics
    â”‚
    â””â”€â”€ utils/
        â””â”€â”€ utils/
            â”œâ”€â”€ config.py        # Configuration and hyperparameters
            â”œâ”€â”€ io.py            # Data loading and I/O utilities
            â”œâ”€â”€ logger.py        # Logging setup
            â”œâ”€â”€ mlflow_utils.py  # MLflow integration
            â”œâ”€â”€ kaggle.py        # Kaggle Interface
tests/
â””â”€â”€ tests/
    â””â”€â”€ __init__.py               # Unit test definitions
```
---

# ğŸš€ Getting Started

## 0. Pre-requisites
There are several pre-requisites to be able to use this functionality by simply cloning and installing the python package as it is.

* Poetry should be installed in the base python environment. Dev system used conda to build the base working virtual environment and poetry to handle all further dependencies.

* To use mlflow with a cloud backend - an object store and the remote database needs to be created.
    * This project is using AWS RDS (using postgres) as the backend database and AWS S3 as the artifact store.
    * To reproduce the same, AWS credentials need to be pre-configured inside the env either via aws cli or via .env file.
    * Postgres db connection string needs to be provided
    * For using another backend store natively compatible with mlflow, please pass their db connection string and backend store url

## 1. Clone the Repository
```
git clone https://github.com/mandrake-bio/mlutils
cd mlutils

poetry install

./mlflow.sh

```


# ğŸ§ª Running Tests
Use pytest to run all unit tests:
```
pytest src/tests/
```

For test coverage:
```
pytest --cov=mlutils tests/
```



# ğŸ§° Development Practices
This repository follows strict development standards:

Git version control

Pre-commit hooks for linting, formatting, and secrets

Docker for repeatable environments

MLflow for reproducible ML experiments

Logging to track events

Unit tests for all major components.

# ğŸ“ Notebooks & Data
Jupyter notebooks for data exploration and EDA are in the /notebooks directory.

Training and test datasets should be placed in /data.

Models are saved to /models.

None of these folders are put into .gitignore purposefully to make this entire notebook reproducible.

# ğŸ¤ Contributing

Contributions are welcome!

Fork the repo

Create your feature branch (git checkout -b feature/AmazingFeature)

Commit your changes (git commit -m 'Add amazing feature')

Push to the branch (git push origin feature/AmazingFeature)

Open a Pull Request

Ensure you follow the code standards and include relevant unit tests.

# ğŸ“„ License
Distributed under the MIT License. See LICENSE for more information.

# ğŸ™Œ Acknowledgments
This project integrates tools and practices from:

Scikit-learn

Flask

MLflow

Docker

Pandas / NumPy

Pytest

Pre-commit

GitHub Actions (optional CI/CD integration)

**This file is completely self-contained and fully detailed. You can copy this directly into your project as `README.md` without needing any additional documents. Let me know if you'd like me to generate a badge section or add sample config templates in-code!**
